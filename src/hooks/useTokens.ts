import { useState, useCallback } from 'react';
import { useAuth } from '@/contexts/AuthContext';
import { supabase } from '@/integrations/supabase/client';
import { useToast } from '@/hooks/use-toast';

// Model token costs
const MODEL_COSTS = {
  // Synergy model
  'synergy-ia': 5000,
  // Claude 4 models (Anthropic)
  'claude-opus-4-20250514': 15000,
  'claude-sonnet-4-20250514': 10000,
  'claude-3-5-haiku-20241022': 2500,
  'grok-4-0709': 15000,
  'grok-3': 10000,
  'grok-3-mini': 3000,
  // OpenAI Models baseado na documentação oficial
  'gpt-5': 25000, // $1.25 entrada + $10 saída (média)
  'gpt-5-mini': 5625, // $0.25 entrada + $2 saída (média)
  'gpt-5-nano': 1125, // $0.05 entrada + $0.4 saída (média)
  'gpt-4.1': 15000, // $3 entrada + $12 saída (ajuste fino)
  'gpt-4.1-mini': 4000, // $0.8 entrada + $3.2 saída (ajuste fino)
  'gpt-4.1-nano': 1000, // $0.2 entrada + $0.8 saída (ajuste fino)
  'o4-mini': 8000, // $4 entrada + $16 saída (ajuste fino)
  'claude-3-haiku-20240307': 2000,
  // Google Gemini Models
  'gemini-2.5-pro-002': 12000,
  'gemini-2.5-flash-002': 4000,
  'gemini-2.5-flash-lite-001': 1500,
  // DeepSeek Models
  'deepseek-chat': 8000,
  'deepseek-reasoner': 12000,
  // APILLM Models
  'Llama-4-Maverick-17B-128E-Instruct-FP8': 8000,
  'Llama-4-Scout-17B-16E-Instruct-FP8': 6000,
} as const;

export const useTokens = () => {
  const { user, profile, refreshProfile } = useAuth();
  const { toast } = useToast();
  const [checking, setChecking] = useState(false);

  const getTokenCost = useCallback((modelName: string): number => {
    // Default cost for unknown models
    return MODEL_COSTS[modelName as keyof typeof MODEL_COSTS] || 5000;
  }, []);

  const checkTokenBalance = useCallback(async (modelName: string): Promise<boolean> => {
    if (!user || !profile) {
      toast({
        title: "Erro",
        description: "Você precisa estar logado para usar este recurso.",
        variant: "destructive",
      });
      return false;
    }

    const cost = getTokenCost(modelName);
    
    if (profile.tokens_remaining < cost) {
      toast({
        title: "Tokens insuficientes",
        description: `Você precisa de ${cost.toLocaleString()} tokens, mas possui apenas ${profile.tokens_remaining.toLocaleString()}.`,
        variant: "destructive",
      });
      return false;
    }

    return true;
  }, [user, profile, getTokenCost, toast]);

  const consumeTokens = useCallback(async (modelName: string | undefined, message: string): Promise<boolean> => {
    if (!user || !profile) return false;
    
    // If no model selected, show error
    if (!modelName) {
      toast({
        title: "Modelo não selecionado",
        description: "Por favor, selecione um modelo de IA antes de enviar uma mensagem.",
        variant: "destructive",
      });
      return false;
    }

    setChecking(true);
    
    try {
      const cost = getTokenCost(modelName);
      
      // Check if user has enough tokens
      if (profile.tokens_remaining < cost) {
        toast({
          title: "Tokens insuficientes",
          description: `Você precisa de ${cost.toLocaleString()} tokens, mas possui apenas ${profile.tokens_remaining.toLocaleString()}.`,
          variant: "destructive",
        });
        return false;
      }

      // Deduct tokens from user profile
      const { error: updateError } = await supabase
        .from('profiles')
        .update({ 
          tokens_remaining: profile.tokens_remaining - cost 
        })
        .eq('id', user.id);

      if (updateError) {
        console.error('Error updating tokens:', updateError);
        toast({
          title: "Erro",
          description: "Não foi possível consumir os tokens. Tente novamente.",
          variant: "destructive",
        });
        return false;
      }

      // Log token usage
      await supabase
        .from('token_usage')
        .insert({
          user_id: user.id,
          model_name: modelName,
          tokens_used: cost,
          message_content: message.substring(0, 1000), // Limit message length
        });

      // Refresh profile to update token count
      await refreshProfile();

      return true;
    } catch (error) {
      console.error('Error consuming tokens:', error);
      toast({
        title: "Erro",
        description: "Ocorreu um erro ao processar os tokens.",
        variant: "destructive",
      });
      return false;
    } finally {
      setChecking(false);
    }
  }, [user, profile, getTokenCost, refreshProfile, toast]);

  const getModelDisplayName = useCallback((modelName: string): string => {
    const displayNames: Record<string, string> = {
      // Synergy
      'synergy-ia': 'SynergyIA',
      // OpenAI Models
      'gpt-5': 'GPT-5',
      'gpt-5-mini': 'GPT-5 Mini', 
      'gpt-5-nano': 'GPT-5 Nano',
      'gpt-4.1': 'GPT-4.1',
      'gpt-4.1-mini': 'GPT-4.1 Mini',
      'gpt-4.1-nano': 'GPT-4.1 Nano', 
      'o4-mini': 'o4 Mini',
      'claude-opus-4-20250514': 'Claude Opus 4',
      'claude-sonnet-4-20250514': 'Claude Sonnet 4',
      'claude-3-5-haiku-20241022': 'Claude 3.5 Haiku',
      'grok-4-0709': 'Grok 4',
      'grok-3': 'Grok 3', 
      'grok-3-mini': 'Grok 3 Mini',
      'claude-3-haiku-20240307': 'Claude 3 Haiku',
      // Google Gemini Models
      'gemini-2.5-pro-002': 'Gemini 2.5 Pro',
      'gemini-2.5-flash-002': 'Gemini 2.5 Flash',
      'gemini-2.5-flash-lite-001': 'Gemini 2.5 Flash-Lite',
      // DeepSeek Models
      'deepseek-chat': 'DeepSeek Chat V3',
      'deepseek-reasoner': 'DeepSeek Reasoner',
      // APILLM Models
      'Llama-4-Maverick-17B-128E-Instruct-FP8': 'Llama 4 Maverick',
      'Llama-4-Scout-17B-16E-Instruct-FP8': 'Llama 4 Scout',
    };
    
    return displayNames[modelName] || modelName;
  }, []);

  return {
    tokenBalance: profile?.tokens_remaining || 0,
    getTokenCost,
    checkTokenBalance,
    consumeTokens,
    getModelDisplayName,
    checking,
  };
};